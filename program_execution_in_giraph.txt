##########################################################################################################################################
						running and executing programs in giraph	##########################################################################################################################################

install hadoop (install namenode, datanode, secondarynamenode, jobtracker, task tracker, conf-pseudo)
install hadoop-zookeeper-server

##########################################################################################################################################

add following properties to mapred-site.xml 
-------------------------------------------
1) counter limit by default is 120, as some job may exceed this limit so its better to keep safe limit (give some value around 512 not necessarily 1024 (mostly will never hit even 512 limit))

  <property>
    <name>mapred.map.tasks</name>
    <value>4</value>
  </property>

  <property>
    <name>mapreduce.job.counters.limit</name>
    <value>1024</value>
  </property>


##########################################################################################################################################

format namenode 
sudo -u hdfs hadoop namenode -format

start all services
eg:
	sudo service hadoop-0.20-namenode start
	sudo serveice hadoop-zookeeper-server start

#############################################################################################################################################
install maven 3.* (ie maven 3 or higher)
#############################################################################################################################################

there are two versions available released 1.0 and unreleased 2.0

download giraph incubator release 1.0 manually or ( 2.0 from git using following command-  git clone git://github.com/apache/giraph.git)
cd giraph
mvn -Phadoop_non_secure clean install (will create all jar)
#############################################################################################################################################
or
execute command for skiping test and building giraph jars with depenedencies:
-----------------------------------------------------------------------------
mvn -Phadoop_non_secure test -DskipTests
or
mvn -Phadoop_non_secure clean test -DskipTests

jar file's will be in /target folder

#############################################################################################################################################
					Note: Please ensure you read and write permission on hdfs
#############################################################################################################################################
then execute sample example
---------------------------

download shortestpath sample input
untar using :
tar zxvf shortestPathsInputGraph.tar.gz 

load file to hadoop fs using:
hadoop fs -copyFromLocal shortestPathsInputGraph shortestPathsInputGraph

Note: *	Sample Input shortestpath is in json format
      * Check if the input is copied, correctly in hdfs(else might create exception while executing)	

#############################################################################################################################################

execute shortest path example using following:
----------------------------------------------
giraph 0.1
--------------
SimpleShortestPaths :
---------------------------------------
hadoop jar target/giraph-0.70-jar-with-dependencies.jar org.apache.giraph.examples.SimpleShortestPathsVertex shortestPathsInputGraph shortestPathsOutputGraph 0 3

giraph 0.2
--------------
SimpleShortestPaths :
---------------------------------------
hadoop jar target/giraph-0.2-SNAPSHOT-for-hadoop-0.20.2-jar-with-dependencies.jar org.apache.giraph.GiraphRunner org.apache.giraph.examples.SimpleShortestPathsVertex -vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat -vip shortestPathsInputGraph -of org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexOutputFormat -op shortestPathsOutputGraph1 -w 1

#############################################################################################################################################
Executing other programs:
-------------------------
Closeness :
---------------------------------------
hadoop jar target/giraph-0.2-SNAPSHOT-jar-with-dependencies.jar org.apache.giraph.examples.closeness.FMClosenessVertex closenessInputGraph closenessOutputGraph 32 3

bc_random:
--------------
hadoop jar target/giraph-0.2-SNAPSHOT-for-hadoop-0.20.2-jar-with-dependencies.jar org.apache.giraph.examples.bc_random org.apache.giraph.examples.bc_randomVertex -_GMInputFormat ADJ org.apache.giraph.examples.bc_randomVertexInputFormat -_GMOutputFormat ADJ org.apache.giraph.examples.bc_randomVertexOutputFormat --K 5 -i inputbet -o out -w 1 -v

degree counts :
--------------
hadoop jar degree_spath.jar org.apache.giraph.examples.BothDegree shortestPathsInputGraph in 1
hadoop jar degree_spath.jar org.apache.giraph.examples.InDegree shortestPathsInputGraph in 1
hadoop jar degree_spath.jar org.apache.giraph.examples.OutDegreeCount shortestPathsInputGraph in 1

Master compute example
--------------
hadoop jar target/giraph-0.2-SNAPSHOT-for-hadoop-0.20.203.0-jar-with-dependencies.jar org.apache.giraph.GiraphRunner org.apache.giraph.examples.SimpleMasterComputeVertex -vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat -vip /user/hadoop/masterComputeInput -of org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexOutputFormat -op /user/hadoop/masterComputeOutput -w 3 -mc org.apache.giraph.examples.SimpleMasterComputeVertex\$SimpleMasterCompute -wc org.apache.giraph.examples.SimpleMasterComputeVertex\$SimpleMasterComputeWorkerContext

#############################################################################################################################################
check output like this:
-----------------------
hadoop fs -ls shortestPathsOutputGraph
Found 5 items
-rw-r--r--   1 aching supergroup          0 2011-08-02 13:20 /user/aching/shortestPathsOutputGraph/_SUCCESS
drwxr-xr-x   - aching supergroup          0 2011-08-02 13:20 /user/aching/shortestPathsOutputGraph/_logs
-rw-r--r--   1 aching supergroup         96 2011-08-02 13:20 /user/aching/shortestPathsOutputGraph/part-m-00001
-rw-r--r--   1 aching supergroup        109 2011-08-02 13:20 /user/aching/shortestPathsOutputGraph/part-m-00002
-rw-r--r--   1 aching supergroup         84 2011-08-02 13:20 /user/aching/shortestPathsOutputGraph/part-m-00003

hadoop fs -cat shortestPathsOutputGraph/part*
[5, 1000, [ [6, 500] ] ]
[6, 1500, [ [7, 600] ] ]
[7, 2100, [ [8, 700] ] ]
[8, 2800, [ [9, 800] ] ]
[9, 3600, [ [10,900] ] ]
[10, 4500, [ [11, 1000] ] ]
[11, 5500, [ [12, 1100] ] ]
[12, 6600, [ [13, 1200] ] ]
[13, 7800, [ [14, 1300] ] ]
[14, 9100, [ [0, 1400] ] ]
[0, 0, [ [1, 0] ] ]
[1, 0, [ [2, 100] ] ]
[2, 100, [ [3, 200] ] ]
[3, 300, [ [4, 300] ] ]
[4, 600, [ [5, 400] ] ]

#############################################################################################################################################
Note : if job fails cleanup process might take time so, can kill the job manually
Killing hadoop job
-------------------
hadoop job -kill <job_id>
eg:
hadoop job -kill job_201302041058_0005

kill normal job:
----------------
sudo kill <pid>

###########################################################################################################################################
						Getting output data into hive
						   -------------------------
0)	sudo -u hdfs hive

1)	Hive Create table:
	create table giraphdegree     (id int , dval float) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';
	create table giraphbetweenness(id int , dval float) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';
	create table giraphcloseness  (id int , dval float) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';

2)	exit;

3)	Remove the table folder from hdfs
	sudo -u hdfs hadoop fs -rmr /user/hive/warehouse/giraphdegree
	sudo -u hdfs hadoop fs -rmr /user/hive/warehouse/giraphbetweenness
	sudo -u hdfs hadoop fs -rmr /user/hive/warehouse/giraphcloseness

4)	Upload data in hdfs in following format
	<nodeid> <nodevalue> <edge<targetid1 targetvalue1> <targetid2 targetvalue2>>
	ex:	1 0 2 1 3 1

	sudo -u hdfs hadoop fs -copyFromLocal inputDirName inputDirName

5)	run the command for programs
Degree
-------
hadoop jar giraph-0.1-jar-with-dependencies.jar org.apache.giraph.examples.BothDegree inputDirName /user/hive/warehouse/giraphdegree 1

Betweeness
----------
hadoop jar target/giraph-0.2-SNAPSHOT-for-hadoop-0.20.2-jar-with-dependencies.jar org.apache.giraph.examples.bc_random org.apache.giraph.examples.bc_randomVertex -_GMInputFormat ADJ org.apache.giraph.examples.bc_randomVertexInputFormat -_GMOutputFormat ADJ org.apache.giraph.examples.bc_randomVertexOutputFormat --K 5 -i inputDirName -o /user/hive/warehouse/giraphbetweenness -w 1 -v

Closeness
---------
hadoop jar target/giraph-0.2-SNAPSHOT-jar-with-dependencies.jar org.apache.giraph.examples.closeness.FMClosenessVertex inputDirName /user/hive/warehouse/giraphcloseness 32 3

6)	goto hive
	sudo -u hdfs hive

7)	select * from giraphdegree;
	select * from giraphbetweenness;
	select * from giraphcloseness;
	
	
-----------------------------------------------------------------------------------------------------------------------------------------

8)	Finally you will have 3 tables degree , closeness and betweeness
	try count(*) from table to see if hive map-reduce is working on hive. if then,

	* use join query to select from three tables or
	* create new table Result : nodeid , deg_value , bet_value , clos_value
	  and insert result of join in this table for use in further processing
	
	sample code for insert with select.
	INSERT INTO tbl_target
	   (
	      SELECT column_a,
	             column_b,
	             column_c
	        FROM tbl_source
	       WHERE condition
	   );

###########################################################################################################################################


